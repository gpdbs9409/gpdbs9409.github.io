---
layout: post
title:  "크롤링기록"
---
1. 맨 처음에는 크롤링을 위해  페이지 소스를 전부본다음 , getbytag같은 함수를 이용해서 데이터를 뽑아오려고 했다.  

[블로그 체험단 포블로그!](https://4blog.net/list/all/local/seoul/hdss)

아래는 포블로그 사이트 소스 일부 . 

각 식당정보를 item으로 처리한 듯하다 . 

item.Remaindate 

item. category

item.CAMPAIGN_NM 등 

— 얘네가 우리가 긁어와야할 데이터임 . 

```jsx
function appendDataToContainer(data) {
		// 가져온 데이터를 반복 처리합니다.
		data.forEach(function (item) {
			// 각 데이터 항목에 대한 HTML 생성

			const categoryClass = getCategoryClass(item);
			const category = getCategory(item);
			const PointAndDateHtml = getPointAndDateHtml(item);

			const html = `
					 <a class="nounderline" href="/campaign/`+item.CID+`/" target="_blank">
						 <div class="mainbox">
							 <div class="main-img-div">
								 <img src="https://d3oxv6xcx9d0j1.cloudfront.net/public/pr/`+item.PRID+`/thumbnail/`+item.IMGKEY+`" class="img-rounded main-img-inbox lazy" />
								 <noscript>
									 <img src="https://d3oxv6xcx9d0j1.cloudfront.net/public/pr/`+item.PRID+`/thumbnail/`+item.IMGKEY+`" class="img-rounded main-img-inbox lazy" />
								 </noscript>
							 </div>
							<div class="camp-description">
								<span class="camp-name">`+item.CAMPAIGN_NM+`</span>
								 <div class="emphasize">`+item.REVIEWER_BENEFIT+`</div>
								<div class="border"></div>
								<div class="remainInfo">
									<span class="`+categoryClass+`" style="float:left">`+category+`</span>
								</div>
								<div class="point">
									`+PointAndDateHtml+`
									</span>
								</div>
							</div>
						 </div>
					 </a>
				 `;
			// 생성한 HTML을 컨테이너에 추가
			const container = document.getElementById('rCampaignContainer');
			container.insertAdjacentHTML('beforeend', html);

			// $("img.lazy").lazyload({
			//  placeholder : "/resources/img/empty.gif",
			//  threshold : 30,
			// });

		});
	}
```

---

1. 그렇지만  , 이렇게 소스코드로는 데이터를 뽑아올순 없었다 . 브라우저에 show되는 요소들이 소스코드에 없고 . 포블로그가 사용하는 서버 내부에 저장되어 있고 , 동적으로 요소들을 불러와야 했기 때문임 . 

3.gpt등 인터넷에 검색해보니 , 파이썬라이브러리 beaufifulsoup을 사용해야한다고함 .  해당 라이브러리 4월에 사용한 적 있고  , 실제로 그 때 사용했던 bs4 그대로 써보니 돌아갔다 . - 네이버 뉴스에서 불러왔음 . 

1. 그런데 4blog사이트에서는 안먹혀서 보니까 , bs4또한 정적화면에만 유효하다고 하고 , 동적화면에서는 이에 추가적으로 selenuim 라이브러리와 , request를 같이 써야한다고 한다 . 그러니까 나도 해당 서버에 접근해서 동적으로 파일을 받아와야한다는 의미인듯. 

투트랙으로 가야하는 것이다 .

1. 정적화면은 html
2. 동적화면은 requests+ selenium

[[핀테크/데이터분석] week08_web crawling#01 / 동적, 정적  / 동기, 비동기 sync async / 웹크롤링 / requests, beautifulsoup](https://blog.naver.com/quitecredible/223156800713)

 

 5. 아나콘다 프롬프트를 이용하여 seleium 설치 완료 . 

[데이터 크롤링: Selenium으로 동적 웹페이지 다루기 🌐🤖](https://blog.naver.com/quantshow/223186788914)

이제 얘를 이용해서 크롤링 해오는 것만 남음 . 

하아아 .. AJAX URL을 알아야한다고 하는데.. 

이걸어떻게 아냐,, 일단 개발자도구 - NETWORK 탭에서 HAR 파일을 다운받음 . (HAR파일은 HTTP 아카이브 파일  .   웹 사이트 방문시 생성된 모든 HTTP요청과 응답에 대한 정보가 포함되어있음.  ) 

[4blog.net.har](https://prod-files-secure.s3.us-west-2.amazonaws.com/0dbe2233-315d-4371-a466-dfb578b52bdf/775e788b-f9fa-436c-b7d3-1f9de05fa26f/4blog.net.har)

*아젠다는 동적 웹페이지 크롤링 ! 

이 har
