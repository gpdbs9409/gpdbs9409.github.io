---
title: SPARKS
layout: post
categories:
  - BigData
---
자연어처리 : word2vec
자연어가 뭘까 ? 말 그대로 자연스러운 언어 . 생활 속에서 사용하는 언어이다 . 내가 지금 작성하고 있는 것도 자연어다 . . 그렇다면 , 처리는 뭘까 ? 영어로 하는 게 낫겠다 . process . 
자연어를 컴퓨터가 처리 해주는 거다 . 흔히 컴퓨터는 0과1 의 딱딱함이라고 생각하지만 , 그렇게 안 하구 ..

단어 사이의 연관성을 표현한다 . 



여튼 자연어 처리는 여기까지 하고 

word Embedding은 , one hot - encoding 의 문제를 보완하고자 하면서 나온 문제이다 . one-hot-encoding의 문제는 뭐였냐면 .. 
그 "1"을 표현하기 위해서는 나머지 공간이 0으로 채워져야한다 . 얘네는 배경이니까 .(sparse representation) 근데 그 0으로 채워진 공간이 너무나 낭비였던 것이다 . 

그래서 
밀집 표현( dense representation)을 이용하자 
라고 나옴 -> word Embedding 

어떻게 ? 벡터의 차원을 원하는 대로 설정할 수 있음 .. 

그럼 여기서 벡터의 차원이라는건 뭐였지 ?(팩터 하나에 차원 하나로 치환시키면 됨 . )


젤 중요한건 단어 유사도 ~ 
유사도는 어떻게 따질까 ? 

cosine 유사도 , Euclidean 유사도 . 


유사도는 언제 높은데 ? 
1. 한 문단안에 자주 등장하는 단어들끼리 상관관계가 높겠다 . 

CBOW  (주변 단어를 이용해 중간단어 예측 )
Skip-Gram(중간 단어를 활용해 주변에 있는 단어를 예측)