---
layout: post
title: selenium 사용기(동적크롤링 성공!)
categories:
  - Web programming
---


[[bs4]]는 html 소스를 가져오고 (이건 request가 하는 일 ), 여기서 그 데이터를 내 입맛대로 바꾸는 것을 해줌(이게 bs4역할 ) 근데 동적크롤링은 그걸로 안 됨

https://blog.naver.com/leebisu/222752506291
실습주소
Chrome이 최신 버전입니다.

버전 120.0.6099.71(공식 빌드) (64비트)Chrome이 최신 버전입니다.
> 버전에 맞춘 크롬 웹 드라이버 다운 받음 

버전 120.0.6099.71(공식 빌드) (64비트)

`from selenium import webdriver from selenium.webdriver.common.by import By from bs4 import BeautifulSoup import math driver = webdriver.Chrome("C:/Users/doomoolmori/Dropbox/My Book/quant_python/chromedriver.exe")
`

크롤링 성공했던 
방식 
1. https://blog.naver.com/leebisu/222752506291
이것과 동일하되 , 크롬 버전 맞추는 것 조심하기 

2. 크롬태스크창으로 조절하기 

아래 코드는 
강남맛집 , 포블로그 두 사이트의 각 페이지를 리스트로 뽑은 다음 (중간중간 webdriverwait를 사용해줘야 함 . )

````python
````from selenium import webdriver
from selenium.webdriver.common.by import By

from bs4 import BeautifulSoup
import math 


from selenium import webdriver

# Set ChromeDriver executable path
chrome_driver_path = 'C:/Users/gpdbs/Desktop/chromedriver-win64/chromedriver.exe'

# Create ChromeOptions object
chrome_options = webdriver.ChromeOptions()

# Add any desired options to the ChromeOptions object
# For example, you can add options to run Chrome in headless mode:
# chrome_options.add_argument('--headless')

# Specify the path to the ChromeDriver executable within options
chrome_options.add_argument(f'--webdriver-path={chrome_driver_path}')

# Initialize the WebDriver with the specified options
driver = webdriver.Chrome(options=chrome_options)

# Now you can use the 'driver' object to interact with the web page.


url = 'https://xn--939au0g4vj8sq.net/cp/?ca=2005&loca_prt=&local_1=&local_2='
driver.get(url)



html = BeautifulSoup(driver.page_source, 'lxml')


data_name = html.find_all(class_ = 'tit')
[i.get_text() for i in data_name]

datalist = [i.text for i in data_name]  # Store the text in datalist

# Now, datalist contains the list of strings you extracted
print(datalist)

from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

url = 'https://4blog.net/list/all/reporter'
driver.get(url)

WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.CLASS_NAME, "camp-name"))
)

html = BeautifulSoup(driver.page_source, 'lxml')



data_name = html.find_all(class_ = 'camp-name')
[i.get_text() for i in data_name]

datalist.extend(i.get_text()for i in data_name)
print(datalist)````




```

```ouput
['[맘카페 기자단] 미.면.정 서.현.점 (분당)',
 '[네이버카페 기자단] 스.터.디.카.페',
 '[인스타기자단] 대.구.패.대.기',
 '[카카오리뷰] 오.백.돈.고.기.집',
 '[기자단] 나나방콕 수완점',
 '[네이버카페 기자단] 양.키.스.그.릴.',
 '[기자단]  블로그 홍보기자단  [5000 포인트]',
 '[구글리뷰] 두.리.원',
 '[인스타기자단] 태.호.숯.불.갈.비',
 '[네이버카페 기자단] 스.터.디.카.페',
 '[배포기자단] 오.프.레.뷰.티',
 '[영수증리뷰] 쿠우쿠우목포점',
 '[기자단] 나나방콕 상무점']
 ['[네이버카페 기자단] 양.키.스.그.릴.',
 '[인스타기자단] 대.구.패.대.기',
 '[카카오리뷰] 오.백.돈.고.기.집',
 '[구글리뷰] 두.리.원',
 '[인스타기자단] 태.호.숯.불.갈.비',
 '[맘카페 기자단] 미.면.정 서.현.점 (분당)',
 '[기자단] 나나방콕 수완점',
 '[네이버카페 기자단] 스.터.디.카.페',
 '[기자단] 나나방콕 상무점',
 '[배포기자단] 오.프.레.뷰.티',
 '[영수증리뷰] 쿠우쿠우목포점',
 '[네이버카페 기자단] 스.터.디.카.페',
 '[기자단]  블로그 홍보기자단  [5000 포인트]']
```
